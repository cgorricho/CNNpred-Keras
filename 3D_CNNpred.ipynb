{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e332e437",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import scale\n",
    "from os.path import join\n",
    "from sklearn.metrics import accuracy_score as accuracy, f1_score, mean_absolute_error as mae\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D\n",
    "from pathlib2 import Path\n",
    "from tensorflow.keras import backend as K, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddfcb20",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision_pos = precision(y_true, y_pred)\n",
    "    recall_pos = recall(y_true, y_pred)\n",
    "    precision_neg = precision((K.ones_like(y_true)-y_true), (K.ones_like(y_pred)-K.clip(y_pred, 0, 1)))\n",
    "    recall_neg = recall((K.ones_like(y_true)-y_true), (K.ones_like(y_pred)-K.clip(y_pred, 0, 1)))\n",
    "    f_posit = 2*((precision_pos*recall_pos)/(precision_pos+recall_pos+K.epsilon()))\n",
    "    f_neg = 2 * ((precision_neg * recall_neg) / (precision_neg + recall_neg + K.epsilon()))\n",
    "\n",
    "    return (f_posit + f_neg) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299dc281",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_data(file_fir):\n",
    "    try:\n",
    "        df_raw = pd.read_csv(file_fir, parse_dates=['Date'])\n",
    "        df_raw.index = df_raw['Date']\n",
    "    except IOError:\n",
    "        print(\"IO ERROR\")\n",
    "    return df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a679bc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def construct_data_warehouse(ROOT_PATH, file_names):\n",
    "    global number_of_stocks\n",
    "    global samples_in_each_stock\n",
    "    global number_feature\n",
    "    global predict_index\n",
    "    global order_stocks\n",
    "    tottal_train_data = np.empty((0,82))\n",
    "    tottal_train_target = np.empty((0))\n",
    "    tottal_test_data = np.empty((0,82))\n",
    "    tottal_test_target = np.empty((0))\n",
    "\n",
    "    for stock_file_name in file_names:\n",
    "\n",
    "        file_dir = os.path.join(ROOT_PATH, stock_file_name)\n",
    "        ## Loading Data\n",
    "        try:\n",
    "            df_raw = load_data(file_dir)\n",
    "        except ValueError:\n",
    "            print(\"Couldn't Read {} file\".format(file_dir))\n",
    "\n",
    "        number_of_stocks += 1\n",
    "\n",
    "        data = df_raw\n",
    "        df_name = data['Name'][0]\n",
    "        order_stocks.append(df_name)\n",
    "        del data['Name']\n",
    "\n",
    "        target = (data['Close'][predict_day:] / data['Close'][:-predict_day].values).astype(int)\n",
    "        data = data[:-predict_day]\n",
    "        target.index = data.index\n",
    "        # Becasue of using 200 days Moving Average as one of the features\n",
    "        data = data[200:]\n",
    "        data = data.fillna(0)\n",
    "        data['target'] = target\n",
    "        target = data['target']\n",
    "        del data['target']\n",
    "        del data['Date']\n",
    "        # data['Date'] = data['Date'].apply(lambda x: x.weekday())\n",
    "\n",
    "        number_feature = data.shape[1]\n",
    "        samples_in_each_stock = data.shape[0]\n",
    "\n",
    "        train_data = data[data.index < '2016-04-21']\n",
    "        train_data = scale(train_data)\n",
    "\n",
    "        if df_name == predict_index:\n",
    "            tottal_train_target = target[target.index < '2016-04-21']\n",
    "            tottal_test_target = target[target.index >= '2016-04-21']\n",
    "\n",
    "        data = pd.DataFrame(scale(data.values), columns=data.columns)\n",
    "        data.index = target.index\n",
    "        test_data = data[data.index >= '2016-04-21']\n",
    "\n",
    "        tottal_train_data = np.concatenate((tottal_train_data, train_data))\n",
    "        tottal_test_data = np.concatenate((tottal_test_data, test_data))\n",
    "\n",
    "    train_size = int(tottal_train_data.shape[0]/number_of_stocks)\n",
    "    test_size = int(tottal_test_data.shape[0] / number_of_stocks)\n",
    "    tottal_train_data = tottal_train_data.reshape(number_of_stocks, train_size, number_feature)\n",
    "    tottal_test_data = tottal_test_data.reshape(number_of_stocks, test_size, number_feature)\n",
    "\n",
    "\n",
    "    return tottal_train_data, tottal_test_data, tottal_train_target, tottal_test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6230d0dd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def cnn_data_sequence(data, target, seque_len):\n",
    "    print ('sequencing data ...')\n",
    "    new_train = []\n",
    "    new_target = []\n",
    "\n",
    "    for index in range(data.shape[1] - seque_len + 1):\n",
    "        new_train.append(data[:, index: index + seque_len])\n",
    "        new_target.append(target[index + seque_len - 1])\n",
    "\n",
    "    new_train = np.array(new_train)\n",
    "    new_target = np.array(new_target)\n",
    "\n",
    "    return new_train, new_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa54197",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def sklearn_acc(model, test_data, test_target):\n",
    "    overall_results = model.predict(test_data)\n",
    "    test_pred = (overall_results > 0.5).astype(int)\n",
    "    acc_results = [mae(overall_results, test_target), accuracy(test_pred, test_target),\n",
    "                   f1_score(test_pred, test_target, average='macro')]\n",
    "\n",
    "    return acc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5ebe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN(train_data, test_data, train_target, test_target):\n",
    "    # hisory of data in each sample\n",
    "    seq_len = 60\n",
    "    epoc = 100\n",
    "    drop = 0.1\n",
    "\n",
    "    # creating sample each containing #seq_len history\n",
    "    cnn_train_data, cnn_train_target = cnn_data_sequence(train_data, train_target, seq_len)\n",
    "    cnn_test_data, cnn_test_target = cnn_data_sequence(test_data, test_target, seq_len)\n",
    "    result = []\n",
    "\n",
    "    # Running CNNpred several times\n",
    "    for i in range(1,40):\n",
    "        K.clear_session()\n",
    "        print ('i: ', i)\n",
    "        my_file = Path( join(Base_dir, '3D-models/{}/model/{}-{}-{}-{}-{}.h5'.format(predict_index, epoc, seq_len, number_filter, drop, i)))\n",
    "        filepath = join(Base_dir, '3D-models/{}/model/{}-{}-{}-{}-{}.h5'.format(predict_index, epoc, seq_len, number_filter, drop, i))\n",
    "\n",
    "        # If the trained model doesn't exit, it is trained\n",
    "        if my_file.is_file():\n",
    "            print('loading model')\n",
    "\n",
    "        else:\n",
    "            print('fitting model')\n",
    "            model = Sequential()\n",
    "\n",
    "            #layer 1\n",
    "            model.add(Conv2D(number_filter[0], (1, 1), activation='relu', input_shape=(number_of_stocks,seq_len, number_feature), data_format='channels_last'))\n",
    "            #layer 2\n",
    "            model.add(Conv2D(number_filter[1], (number_of_stocks, 3), activation='relu'))\n",
    "            model.add(MaxPool2D(pool_size=(1, 2)))\n",
    "\n",
    "            #layer 3\n",
    "            model.add(Conv2D(number_filter[2], (1, 3), activation='relu'))\n",
    "            model.add(MaxPool2D(pool_size=(1, 2)))\n",
    "\n",
    "            model.add(Flatten())\n",
    "            model.add(Dropout(drop))\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "            model.compile(optimizer='Adam', loss='mae', metrics=['acc',f1])\n",
    "\n",
    "            best_model = callbacks.ModelCheckpoint(filepath, monitor='val_f1', verbose=0, save_best_only=True,\n",
    "                                                   save_weights_only=False, mode='max', period=1)\n",
    "\n",
    "            model.fit(cnn_train_data, cnn_train_target, epochs=epoc, batch_size=128, verbose=0,callbacks=[best_model], validation_split=0.25)\n",
    "\n",
    "        model = load_model(filepath, custom_objects={'f1': f1})\n",
    "        test_pred = sklearn_acc(model,cnn_test_data, cnn_test_target)\n",
    "        print (test_pred)\n",
    "        result.append(test_pred)\n",
    "\n",
    "    print('saving results')\n",
    "    results = pd.DataFrame(result , columns=['MAE', 'Accuracy', 'F-score'])\n",
    "    results = results.append([results.mean(), results.max(), results.std()], ignore_index=True)\n",
    "    results.to_csv(join(Base_dir, '3D-models/{}/new results.csv'.format(predict_index)), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a6a4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base_dir = ''\n",
    "TRAIN_ROOT_PATH = join(Base_dir, 'Dataset')\n",
    "train_file_names = os.listdir(join(Base_dir, 'Dataset'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a80e4b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# if moving average = 0 then we have no moving average\n",
    "moving_average_day = 0\n",
    "number_of_stocks = 0\n",
    "number_feature = 0\n",
    "samples_in_each_stock = 0\n",
    "number_filter = [8,8,8]\n",
    "predict_day = 1\n",
    "order_stocks = []\n",
    "# Name of the index that is going to be predicted\n",
    "predict_index = 'DJI'   # RUT, S&P, NYA, NASDAQ, DJI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4c62d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Loading train data ...')\n",
    "train_data, test_data, train_target, test_target = construct_data_warehouse(TRAIN_ROOT_PATH, train_file_names)\n",
    "print ('number of stocks = ', number_of_stocks)\n",
    "print ('fitting model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfcbef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN(train_data, test_data, train_target, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac1e9e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b157fd88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5364e86e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f37ace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc719cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
